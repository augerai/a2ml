---
# Name of the DataSet on Auger Cloud
dataset:
# Experiment settings that are unique to Auger
experiment:
  # Latest experiment name
  name:
  # Latest experiment session
  experiment_session_id:
  # Time series feature. If Data Source contains more then one DATETIME feature
  # you will have to explicitly specify feature to use as time series
  time_series:
  # List of columns which should be used as label encoded features
  label_encoded: []

  # List of all supported model names:
  # *Classification: XGBClassifier,LGBMClassifier,LinearSVC,SVC,SGDClassifier,
  # AdaBoostClassifier,DecisionTreeClassifier,ExtraTreesClassifier,RandomForestClassifier,
  # GradientBoostingClassifier,CatBoostClassifier
  # *Regression: SVR,XGBRegressor,LGBMRegressor,ElasticNet,
  # SGDRegressor,AdaBoostRegressor,DecisionTreeRegressor,ExtraTreesRegressor,
  # RandomForestRegressor,GradientBoostingRegressor,CatBoostRegressor

  # A list of model names to ignore for an experiment  
  #blocked_models:  

  # A list of model names to search for an experiment. 
  # If not specified, then all models supported for the task are used minus any specified in blocked_models
  #allowed_models:

  # List of all supported preprocessors:
  # nan, date_time, categorical, text, eliminate, sparse, extra, scale, dim_reduction

  # A list of preprocessors names to ignore for an experiment  
  #blocked_preprocessors:  

  # A list of preprocessors names to apply for an experiment. 
  # If not specified, then all preprocessors supported  are used minus any specified in blocked_preprocessors
  #allowed_preprocessors:

  # Optional. Default: False. 
  # Estimate Trial Time Predict the training time of each individual model to avoid timeouts.
  #estimate_trial_time: False

  # Optional. Default: None. Use it if you have a lot of failed trials
  # Set it to value < 8 to give trial fit process more memory.  
  # trials_per_worker: 2

  # Optional. Default: None. Used in case of unbalanced datasets.
  # Class Weight Weights associated with classes. If None, all classes are supposed to have weight one. 
  # The Balanced mode automatically adjusts weights inversely proportional to class frequencies in the input data. 
  # The Balanced Subsample mode is the same as Balanced except that weights are computed based on the bootstrap sample for every tree grown.
  # class_weight: balanced

  # Oversampling Methods to adjust the class distribution of a data set 
  # (in binary classification if 1-class is 99% and 0-class is 1% then samples from 0-class should be added to dataset in order to prevent overfitting on majority class)
  # Name should be from imbalanced-learn library. For example: SMOTE, RandomOverSampler, ADASYN, SMOTEENN, SMOTETomek
  # Read https://imbalanced-learn.readthedocs.io/en/stable/api.html#module-imblearn.over_sampling
  # For full parameters description  
  # oversampling:
    # name: SMOTE
    # params:
    #   sampling_strategy: minority
    #   k_neighbors: 5

  # Split to folds parameters:
  # Shuffle records in each fold
  #shuffle: True

  # Column to use as groups in split methods. See: https://scikit-learn.org/stable/modules/classes.html#splitter-classes
  #groups_col: data_date

  # Use TimeSeriesSplit for folds generation. Will not change records order on the training data.
  #timeseries_fold_split: False
  # Maximum number of records for test data. Used for timeseries_fold_split
  #test_size_limit: 800

  # Validation data split ratio, used when split to test/validation datasets. If set, cross_validation_folds will be ignored
  #validation_size: 0.8

# CPU cluster hardware settings
cluster:
  # Type could be standard or high_memory
  type: high_memory
  min_nodes: 2
  max_nodes: 6
  stack_version: stable
